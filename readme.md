# Содержание

# Метрические алгоритмы классификации:
* [K ближайших соседей (KNN)](#метод-k-ближайших-соседей-knn)
* [K взвешенных ближайших соседей (KWNN)](#метод-k-взвешанных-соседей-kwnn)
* [Парзеновское окно (PW)](#метод-парзеновского-окна-pw)
* [Потенциальные функции](#)

## Метрические алгоритмы классификации

 Метрические методы обучения - методы основанные на анализе сходства объектов (схожим объектам соответствует схожие ответы).
 
 Для формализации понятия сходства вводится функция расстояния в пространстве.
 
 Метрический алгоритм классификации с обучающей выборкой Xl, относит классифицируемый объкт к тому классу, для которого суммарный вес ближайших обучающих объектов максимален.

 Выбирая весовую функцию можно получать различные метрические классификаторы.
 
### Метод K ближайших соседей (KNN)

Алгоритм К ближайших соседей - KNN относит объект *z* к тому классу, элементов которого больше всего среди k ближайших соседей. 

В задаче используется выборка, содержащая 150 объектов-ирисов:
по 50 объектов каждого из трёх классов. В задаче требуется определить класс любого объекта.


### Алгоритм:
 Сортируется выборка по расстоянию.
 Используется метод KNN который: возвращает класс чаще всего встречается среди k ближайших соседей и присваивает этим ближайшим соседям вес равный единицы.

### Программная реализация алгоритма выглядит следующим образом:
```R
knn <- function(xl, z, k) {	  
    #    функция которая возвращает класс объекта чаще всего встречающейся
	      
      n <- dim(xl)[2] - 1 
      classes <- xl[1:k, n + 1] 
      counts <- table(classes) 
      class <- names(which.max(counts)) 
      return (class)	  
    }
```
 Используется метод скользящего контроля: LOO который определят оптимальное значение k. 
 
 Суть метода:

* Убирается первый объект выборки, и запускается KNN для остальных. 

* Сравнивается класс первого объекта который получился и истиный класс этого же объекта, если они не совпадают, 
то сумма ошибок увеличивается на 1. 

* Проделаваются эти же самые действия для всех остальных объектов выборки тем самым накапливая сумму погрешности. 

* Накопливаемая сумма делится на количество объектов выборки. 

* LOO применяется для каждого k, и то k для которого усредненная ошибка минимальная и будет наше оптимальное k.

### LOO

Определение оптимального значения k:

```R
    loo <- function(xl) {
    #    функция которая возвращает массив средних ошибок
      l <- nrow(xl)
      n <- ncol(xl)
      Sum <- rep(0, l)
        for (i in 1:l){
          z <- xl[i, 1 : (n-1)]
          xl1 <- sort_ojects_by_dist(xl[-i, ], z)		
          for(j in 1:l){
            class <- knn(xl1, z, j)	
            if(xl[i, n] != class) 
              Sum[j] <- Sum[j] + 1/l 	
          }
        }
      return(Sum)
    }

    optimal <- function(loo){
    #    записываем в k индекс минимального значения 
      k <- which.min(loo)
      return(k)
    }
```
 Рисуется knn и Loo.
	
###	График KNN и Loo
![](https://github.com/PavlovaJulia/R_Projects/blob/master/lab2/KNNandLOO.png)

5. График 1NN как частный случай алгоритма KNN, при k = 1.
![](https://github.com/PavlovaJulia/R_Projects/blob/master/lab1/1NN.png)

## Метод k взвешанных соседей (KWNN)

Суть алгоритма: 
Вводится строго убывающая функция весов ![](http://latex.codecogs.com/gif.latex?w%28i%29%20%3D%20q%5E%7Bi%7D) - геометрическая 
прогрессия, где *i* это ранг соседа, а *q* - параметр, который подбирается в LOO.

Тоесть каждому *k*-тому соседу присваивается свой вес таким образом: чем ближе сосед *к* классифицируемому объекту тем больше ему будет присваиваться вес.
А какой именно будет присваиваться вес будет определять параметр *q*. 

Кроме функции веса алгоритм ничем не отличается от алгоритма KNN.

### Программная реализация алгоритма выглядит следующим образом:

```R
kwnn <- function(xl, k, q) {	  
  #	возвращает класс объекта чаще всего встречающейся
  
  n <- ncol(xl)
  classes <- xl[1:k, n] 
  table <- table(classes)
  table[1:length(table) ] <- 0
  for(i in names(table))
    for(j in 1:k) # по j-тым соседям
      if(i == xl[j, n]) # i - классы
        table[i] =  table[i] + q^j # добавляем вес
  class <- names(which.max(table)) 
  return (class)	  
}
```
### LOO KWNN 

```R
loo <- function(xl) {
  #	функция возвращает массив средних ошибок
  l <- nrow(xl)
  n <- ncol(xl)
  value_q <- seq(0.1, 1, 0.1) # перебираем q 
  Sum <- matrix(0, l-1, length(value_q)) # матрица, где k это строки а q - столбцы
  for (i in 1:l){
    z <- xl[i, 1 : (n-1)]
    xl1 <- sort_ojects_by_dist(xl[-i, ], z)		
    for(k in 1:(l-1)){
      int_q <- 1
      for(q in value_q){
       class <- kwnn(xl1, k, q)
       if(xl[i, n] != class) 
        Sum[k, int_q] <- Sum[k, int_q] + 1/l 	
       int_q <- int_q + 1 
      }
    }
  }
  return(Sum)
}


optimal <- function(loo){
  #	находит в матрице минимальное значение и в k записывает индеск стоки а в q индекс столбца
  ind_min_k <- 1
  l <- nrow(loo)
  min <- min(loo[1,])
  for(i in 1:l){
    if(min(loo[i,]) < min) {
      min = min(loo[i,])
      ind_min_k <- i
      
    }
  }
  ind_min_q <- (which.min(loo[ind_min_k,]))/10
  opt <- c(ind_min_k,ind_min_q)
  return(opt)
}
```
	
### График KWNN и Loo 
![](https://github.com/PavlovaJulia/R_Projects/blob/master/lab2/KWNNandLOO.png)

## Метод парзеновского окна (PW)

Рассматривается весовая функция ![](http://latex.codecogs.com/gif.latex?w%28i%29%20%3D%20K%28%5Cfrac%7B%5Crho%28x_%7Bi%7D%2Cz%29%7D%7Bh%7D%29) не как функция от ранга соседа а как функция от расстояния. 

где *К* - функция ядра, а h -  ширина окна. Ядро - произвольная четная функция невозрастающая на [0, +inf). 

Суть метода: алгоритм для классифицируемой точки *z* строит окружность, радиусом h(ширина окна). Все точки, не попавшие в эту окружность, не рисуются, если функция ядра не является гауссовским. Для остальных, вычисляется вес, суммируется, и класс с наибольшим весом присваивается классифицируемой точке.

Функции ядра бывают 
* Прямоугольное ![](https://camo.githubusercontent.com/a7fd7f2a009cddb610b460b22300a5a9eab873e7/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535436c61726765253230522532387a2532392532302533442532302535436672616325374231253744253742322537442532302535422537437a2537432532302535436c657125323031253544)
* Треугольное ![](https://camo.githubusercontent.com/90e9884466ca65e36d73ed6a717e326123ce573f/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535436c61726765253230542532387a253239253230253344253230253238312532302d2532302537437a25374325323925323025354363646f742532302535422537437a2537432532302535436c657125323031253544)
* Квартическое ![](https://camo.githubusercontent.com/1e5b0ae73cde8fe5b7a3c248bdbddc1639f96404/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535436c61726765253230512532387a2532392532302533442532302535436672616325374231352537442537423136253744253230253238312532302d2532307a253545322532392535453225323025354363646f742532302535422537437a2537432532302535436c657125323031253544)
* Епанечниково ![](https://camo.githubusercontent.com/0cad7a4e41913e389111d61935a76fba1cbff264/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f7376672e6c617465783f2535436c61726765253230452532387a253239253230253344253230253543667261632537423325374425374234253744253230253238312532302d2532307a2535453225323925323025354363646f742532302535422537437a2537432532302535436c657125323031253544)
* Гауссовское ![](http://latex.codecogs.com/gif.latex?%5CLARGE%20%282%5Cpi%29%5E%7B-%5Cfrac%7B1%7D%7B%7D2%7De%5E%7B%28-%5Cfrac%7B1%7D%7B2%7D*r%5E%7B2%7D%29%7D)

### Графики PW

#### Гауссовское ядро

![](https://github.com/PavlovaJulia/R_Projects/blob/master/lab2/PW_Gays.png)

#### Прямоугольное

![](https://github.com/PavlovaJulia/R_Projects/blob/master/lab2/PW_rec.png)

#### Треугольное

![](https://github.com/PavlovaJulia/R_Projects/blob/master/lab2/PW_triangle.png)

#### Квартическое

![](https://github.com/PavlovaJulia/R_Projects/blob/master/lab2/PW_kvar.png)

#### Епанечниково

![]()
