# Содержание

# Метрические алгоритмы классификации:
* K ближайших соседей (KNN)
* K взвешенных ближайших соседей (KWNN)
* Парзеновское окно (PW)
* Потенциальные функции 

## Метрические алгоритмы классификации

 Метрические методы обучения - методы основанные на анализе сходства объектов (схожим объектам соответствует схожие ответы).
 
 Для формализации понятия сходства вводится функция расстояния в пространстве.
 
 Метрический алгоритм классификации с обучающей выборкой Xl, относит классифицируемый объкт к тому классу, для которого суммарный вес ближайших обучающих объектов максимален.

 Выбирая весовую функцию можно получать различные метрические классификаторы.
 
### Метод K ближайших соседей (KNN)

Алгоритм К ближайших соседей - KNN относит объект *z* к тому классу, элементов которого больше всего среди k ближайших соседей. 

В задаче используется выборка, содержащая 150 объектов-ирисов:
по 50 объектов каждого из трёх классов. В задаче требуется определить класс любого объекта.


### Алгоритм:
 Сортируется выборка по расстоянию.
 Используется метод KNN который: возвращает класс чаще всего встречается среди k ближайших соседей и присваивает этим ближайшим соседям вес равный единицы.

### Программная реализация алгоритма выглядит следующим образом:
```R
knn <- function(xl, z, k) {	  
    #    функция которая возвращает класс объекта чаще всего встречающейся
	      
      n <- dim(xl)[2] - 1 
      classes <- xl[1:k, n + 1] 
      counts <- table(classes) 
      class <- names(which.max(counts)) 
      return (class)	  
    }
```
 Используется метод скользящего контроля: LOO который определят оптимальное значение k. 
 
 Суть метода:

* Убирается первый объект выборки, и запускается KNN для остальных. 

* Сравнивается класс первого объекта который получился и истиный класс этого же объекта, если они не совпадают, 
то сумма ошибок увеличивается на 1. 

* Проделаваются эти же самые действия для всех остальных объектов выборки тем самым накапливая сумму погрешности. 

* Накопливаемая сумма делится на количество объектов выборки. 

* LOO применяется для каждого k, и то k для которого усредненная ошибка минимальная и будет наше оптимальное k.

### LOO

Определение оптимального значения k:

    loo <- function(xl) {
    #    функция которая возвращает массив средних ошибок
      l <- nrow(xl)
      n <- ncol(xl)
      Sum <- rep(0, l)
        for (i in 1:l){
          z <- xl[i, 1 : (n-1)]
          xl1 <- sort_ojects_by_dist(xl[-i, ], z)		
          for(j in 1:l){
            class <- knn(xl1, z, j)	
            if(xl[i, n] != class) 
              Sum[j] <- Sum[j] + 1/l 	
          }
        }
      return(Sum)
    }

    optimal <- function(loo){
    #    записываем в k индекс минимального значения 
      k <- which.min(loo)
      return(k)
    }

 Рисуется knn и Loo.
	
###	График KNN и Loo
![](https://github.com/PavlovaJulia/R_Projects/blob/master/lab2/KNNandLOO.png)

5. График 1NN как частный случай алгоритма KNN, при k = 1.
![](https://github.com/PavlovaJulia/R_Projects/blob/master/lab1/1NN.png)

## Метод k взвешанных соседей (KWNN)

Суть алгоритма: 
Вводится строго убывающая функция весов ![](http://latex.codecogs.com/gif.latex?w%28i%29%20%3D%20%5Cfrac%7Bk%20&plus;%201%20-%20i%7D%7Bk%7D) - геометрическая 
прогрессия, где *i* это ранг соседа.

Тоесть каждому *k*-тому соседу присваивается свой вес таким образом: чем ближе сосед *к* классифицируемому объекту тем больше ему будет присваиваться вес. 

Кроме функции веса алгоритм ничем не отличается от алгоритма KNN.

### Программная реализация алгоритма выглядит следующим образом:

    kwnn <- function(xl, k) {	  
	  #	возвращает класс объекта у которого вес больше всего

        n <- ncol(xl)
        table <- table(xl[1:k,n])
        table[1:length(table)] <- 0
        for(i in names(table)){
          for(j in 1:k)
        	  if(i == xl[j,n]) 
                table[i] =  table[i] + (k-j+1)/k
	    }
        class <- names(which.max(table))
        return (class)	  
	}

### График KWNN и Loo 
![](https://github.com/PavlovaJulia/R_Projects/blob/master/lab2/KWNNandLOO.png)

## Метод парзеновского окна (PW)

Рассматривается весовая функция *w(i, z)* не как функция от ранга соседа а как функция от расстояния.
